#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡ä»¶åˆ†æå™¨ - Smart File Analyzer
===================================
ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç›®å½•åˆ†æä¸æŠ¥å‘Šç”Ÿæˆå·¥å…·ï¼Œå±•ç¤ºå¤šæ–¹é¢çš„ç¼–ç¨‹èƒ½åŠ›ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- ğŸ“Š ç›®å½•ç»“æ„æ‰«æä¸åˆ†æ
- ğŸ“ˆ æ–‡ä»¶ç±»å‹ç»Ÿè®¡ä¸å¯è§†åŒ–
- ğŸ” é‡å¤æ–‡ä»¶æ£€æµ‹ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰
- ğŸ“ ä»£ç è¡Œæ•°ç»Ÿè®¡ï¼ˆæ”¯æŒå¤šç§è¯­è¨€ï¼‰
- âš¡ å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
- ğŸ“ é…ç½®æ–‡ä»¶æ”¯æŒ
- ğŸ¨ ASCII å›¾è¡¨ç”Ÿæˆ
- ğŸ“‹ æ—¥å¿—è®°å½•
- ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†
"""

import os
import sys
import hashlib
import json
import logging
import argparse
import threading
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, Counter
from enum import Enum
import mmap


# ==================== æ ¸å¿ƒæ•°æ®ç±» ====================

class FileCategory(Enum):
    """æ–‡ä»¶åˆ†ç±»æšä¸¾"""
    CODE = "code"
    DOCUMENT = "document"
    IMAGE = "image"
    DATA = "data"
    CONFIG = "config"
    OTHER = "other"


@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯æ•°æ®ç»“æ„"""
    path: Path
    size: int
    category: FileCategory
    extension: str
    lines: int = 0
    hash: Optional[str] = None
    is_duplicate: bool = False
    duplicate_of: Optional[Path] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        data = asdict(self)
        data['path'] = str(self.path)
        data['category'] = self.category.value
        if self.duplicate_of:
            data['duplicate_of'] = str(self.duplicate_of)
        return data


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœæ•°æ®ç»“æ„"""
    target_path: Path
    scan_time: datetime
    total_files: int = 0
    total_size: int = 0
    files_by_category: Dict[str, int] = field(default_factory=dict)
    files_by_extension: Dict[str, int] = field(default_factory=dict)
    code_files: List[FileInfo] = field(default_factory=list)
    duplicate_files: List[Tuple[FileInfo, FileInfo]] = field(default_factory=list)
    top_largest_files: List[FileInfo] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'target_path': str(self.target_path),
            'scan_time': self.scan_time.isoformat(),
            'total_files': self.total_files,
            'total_size': self.total_size,
            'files_by_category': self.files_by_category,
            'files_by_extension': dict(self.files_by_extension),
            'code_stats': {
                'total_code_files': len(self.code_files),
                'total_code_lines': sum(f.lines for f in self.code_files),
                'languages': self._extract_languages()
            },
            'duplicate_count': len(self.duplicate_files),
            'top_largest': [f.to_dict() for f in self.top_largest_files[:10]]
        }
    
    def _extract_languages(self) -> Dict[str, int]:
        """æå–ç¼–ç¨‹è¯­è¨€ç»Ÿè®¡"""
        lang_map = {
            '.py': 'Python', '.js': 'JavaScript', '.ts': 'TypeScript',
            '.java': 'Java', '.cpp': 'C++', '.c': 'C', '.cs': 'C#',
            '.go': 'Go', '.rs': 'Rust', '.rb': 'Ruby', '.php': 'PHP',
            '.swift': 'Swift', '.kt': 'Kotlin', '.scala': 'Scala',
            '.html': 'HTML', '.css': 'CSS', '.sql': 'SQL',
            '.sh': 'Shell', '.bat': 'Batch', '.ps1': 'PowerShell'
        }
        counter = Counter()
        for f in self.code_files:
            lang = lang_map.get(f.extension, f.extension.lstrip('.').capitalize())
            counter[lang] += 1
        return dict(counter)


# ==================== é…ç½®ç®¡ç† ====================

@dataclass
class Config:
    """é…ç½®ç±»"""
    max_workers: int = 4
    min_file_size: int = 0
    exclude_dirs: List[str] = field(default_factory=lambda: [
        '.git', '__pycache__', 'node_modules', '.venv', 'venv',
        '.idea', '.vscode', 'dist', 'build', 'target'
    ])
    exclude_extensions: List[str] = field(default_factory=lambda: [
        '.pyc', '.class', '.o', '.obj', '.exe', '.dll', '.so',
        '.dylib', '.zip', '.tar', '.gz', '.rar'
    ])
    enable_duplicate_detection: bool = True
    duplicate_threshold: int = 1024  # è‡³å°‘1KBæ‰æ£€æµ‹é‡å¤
    
    @classmethod
    def from_file(cls, config_path: Path) -> 'Config':
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return cls(**data)
        return cls()
    
    def save(self, config_path: Path) -> None:
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, indent=2, ensure_ascii=False)


# ==================== æ–‡ä»¶åˆ†ç±»å™¨ ====================

class FileCategorizer:
    """æ™ºèƒ½æ–‡ä»¶åˆ†ç±»å™¨"""
    
    # æ‰©å±•åæ˜ å°„è¡¨
    CATEGORY_MAP = {
        FileCategory.CODE: {
            '.py', '.js', '.ts', '.java', '.cpp', '.c', '.cs', '.go',
            '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.m',
            '.h', '.hpp', '.hh', '.hxx', '.lua', '.pl', '.pm', '.r',
            '.jl', '.dart', '.groovy', '.vb', '.ada', '.f', '.f90'
        },
        FileCategory.DOCUMENT: {
            '.md', '.txt', '.rst', '.tex', '.doc', '.docx', '.pdf',
            '.odt', '.rtf', '.wiki', '.org', '.pub'
        },
        FileCategory.IMAGE: {
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp',
            '.ico', '.tiff', '.tif', '.psd', '.ai', '.eps'
        },
        FileCategory.DATA: {
            '.csv', '.json', '.xml', '.yml', '.yaml', '.toml', '.ini',
            '.cfg', '.conf', '.sqlite', '.db', '.parquet', '.feather',
            '.xlsx', '.xls', '.ods', '.avro', '.orc', '.hdf5'
        },
        FileCategory.CONFIG: {
            '.cfg', '.conf', '.config', '.ini', '.env', '.properties',
            '.toml', '.yaml', '.yml', '.json', '.xml'
        }
    }
    
    @classmethod
    def categorize(cls, file_path: Path) -> FileCategory:
        """æ ¹æ®æ‰©å±•ååˆ†ç±»æ–‡ä»¶"""
        ext = file_path.suffix.lower()
        
        for category, extensions in cls.CATEGORY_MAP.items():
            if ext in extensions:
                return category
        
        # ç‰¹æ®Šå¤„ç†ï¼šæŸäº›æ–‡ä»¶å¯èƒ½æ²¡æœ‰æ‰©å±•åä½†æœ‰ç‰¹å¾
        return FileCategory.OTHER


# ==================== æ ¸å¿ƒåˆ†æå™¨ ====================

class SmartFileAnalyzer:
    """æ™ºèƒ½æ–‡ä»¶åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, config: Config, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self._lock = threading.Lock()
        self.file_infos: List[FileInfo] = []
        self.hash_map: Dict[str, List[FileInfo]] = defaultdict(list)
    
    def analyze(self, target_path: Path) -> AnalysisResult:
        """æ‰§è¡Œåˆ†æä¸»æµç¨‹"""
        self.logger.info(f"å¼€å§‹åˆ†æ: {target_path}")
        start_time = time.time()
        
        result = AnalysisResult(
            target_path=target_path,
            scan_time=datetime.now()
        )
        
        if not target_path.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {target_path}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ–‡ä»¶
        self.logger.info("æ­£åœ¨æ‰«ææ–‡ä»¶...")
        all_files = self._collect_files(target_path)
        result.total_files = len(all_files)
        
        # ç¬¬äºŒæ­¥ï¼šå¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶
        self.logger.info(f"æ­£åœ¨åˆ†æ {len(all_files)} ä¸ªæ–‡ä»¶...")
        self._process_files_parallel(all_files)
        
        # ç¬¬ä¸‰æ­¥ï¼šç»Ÿè®¡æ±‡æ€»
        self._aggregate_results(result)
        
        # ç¬¬å››æ­¥ï¼šæ£€æµ‹é‡å¤æ–‡ä»¶
        if self.config.enable_duplicate_detection:
            self.logger.info("æ­£åœ¨æ£€æµ‹é‡å¤æ–‡ä»¶...")
            self._detect_duplicates(result)
        
        # ç¬¬äº”æ­¥ï¼šæ‰¾å‡ºæœ€å¤§æ–‡ä»¶
        self._find_largest_files(result)
        
        elapsed = time.time() - start_time
        self.logger.info(f"åˆ†æå®Œæˆï¼è€—æ—¶: {elapsed:.2f}ç§’")
        
        return result
    
    def _collect_files(self, target_path: Path) -> List[Path]:
        """æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶"""
        files = []
        
        try:
            for root, dirs, filenames in os.walk(target_path):
                # è¿‡æ»¤ç›®å½•
                dirs[:] = [d for d in dirs if d not in self.config.exclude_dirs]
                
                for filename in filenames:
                    filepath = Path(root) / filename
                    
                    # è¿‡æ»¤æ‰©å±•å
                    if filepath.suffix.lower() in self.config.exclude_extensions:
                        continue
                    
                    try:
                        # è¿‡æ»¤å°æ–‡ä»¶
                        if filepath.stat().st_size < self.config.min_file_size:
                            continue
                    except (OSError, PermissionError):
                        continue
                    
                    files.append(filepath)
                    
        except (PermissionError, OSError) as e:
            self.logger.error(f"æ‰«æç›®å½•æ—¶å‡ºé”™: {e}")
        
        return files
    
    def _process_files_parallel(self, files: List[Path]) -> None:
        """å¹¶è¡Œå¤„ç†æ–‡ä»¶"""
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            future_to_path = {
                executor.submit(self._process_single_file, filepath): filepath
                for filepath in files
            }
            
            completed = 0
            for future in as_completed(future_to_path):
                filepath = future_to_path[future]
                try:
                    file_info = future.result()
                    if file_info:
                        with self._lock:
                            self.file_infos.append(file_info)
                except Exception as e:
                    self.logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
                
                completed += 1
                if completed % 100 == 0:
                    self.logger.debug(f"å·²å¤„ç† {completed}/{len(files)} ä¸ªæ–‡ä»¶")
    
    def _process_single_file(self, filepath: Path) -> Optional[FileInfo]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            stat = filepath.stat()
            category = FileCategorizer.categorize(filepath)
            extension = filepath.suffix.lower()
            
            file_info = FileInfo(
                path=filepath,
                size=stat.st_size,
                category=category,
                extension=extension
            )
            
            # è®¡ç®—ä»£ç è¡Œæ•°ï¼ˆä»…å¯¹ä»£ç æ–‡ä»¶ï¼‰
            if category == FileCategory.CODE:
                file_info.lines = self._count_lines(filepath)
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºé‡å¤æ£€æµ‹ï¼‰
            if stat.st_size >= self.config.duplicate_threshold:
                file_info.hash = self._calculate_file_hash(filepath)
                if file_info.hash:
                    with self._lock:
                        self.hash_map[file_info.hash].append(file_info)
            
            return file_info
            
        except (OSError, PermissionError) as e:
            self.logger.debug(f"æ— æ³•è¯»å–æ–‡ä»¶ {filepath}: {e}")
            return None
    
    def _count_lines(self, filepath: Path) -> int:
        """ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
        try:
            with open(filepath, 'rb') as f:
                # ä½¿ç”¨mmapé«˜æ•ˆç»Ÿè®¡
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    return mm.read().count(b'\n') + 1
        except (OSError, PermissionError, ValueError):
            # å›é€€åˆ°æ™®é€šè¯»å–
            try:
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    return sum(1 for _ in f)
            except:
                return 0
    
    def _calculate_file_hash(self, filepath: Path, sample_size: int = 1024 * 1024) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆé‡‡æ ·ä»¥æé«˜æ€§èƒ½ï¼‰"""
        hasher = hashlib.sha256()
        try:
            with open(filepath, 'rb') as f:
                # è¯»å–æ–‡ä»¶å¼€å§‹ã€ä¸­é—´å’Œç»“æŸçš„é‡‡æ ·å—
                file_size = os.path.getsize(filepath)
                chunks = []
                
                if file_size <= sample_size * 3:
                    # å°æ–‡ä»¶ï¼Œå…¨éƒ¨è¯»å–
                    chunks.append(f.read())
                else:
                    # å¤§æ–‡ä»¶ï¼Œé‡‡æ ·
                    chunks.append(f.read(sample_size))
                    f.seek(file_size // 2)
                    chunks.append(f.read(sample_size))
                    f.seek(-sample_size, 2)
                    chunks.append(f.read(sample_size))
                
                for chunk in chunks:
                    hasher.update(chunk)
                
                return hasher.hexdigest()
        except:
            return None
    
    def _detect_duplicates(self, result: AnalysisResult) -> None:
        """æ£€æµ‹é‡å¤æ–‡ä»¶"""
        duplicate_groups = []
        for file_list in self.hash_map.values():
            if len(file_list) > 1:
                duplicate_groups.append(file_list)
        
        for group in duplicate_groups:
            # æ ‡è®°é‡å¤æ–‡ä»¶
            base = group[0]
            for dup in group[1:]:
                dup.is_duplicate = True
                dup.duplicate_of = base.path
                result.duplicate_files.append((base, dup))
    
    def _aggregate_results(self, result: AnalysisResult) -> None:
        """èšåˆåˆ†æç»“æœ"""
        result.total_size = sum(f.size for f in self.file_infos)
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_counter = Counter()
        extension_counter = Counter()
        
        for file_info in self.file_infos:
            category_counter[file_info.category.value] += 1
            extension_counter[file_info.extension] += 1
            
            if file_info.category == FileCategory.CODE:
                result.code_files.append(file_info)
        
        result.files_by_category = dict(category_counter)
        result.files_by_extension = dict(extension_counter)
    
    def _find_largest_files(self, result: AnalysisResult) -> None:
        """æ‰¾å‡ºæœ€å¤§çš„æ–‡ä»¶"""
        sorted_files = sorted(self.file_infos, key=lambda x: x.size, reverse=True)
        result.top_largest_files = sorted_files[:20]


# ==================== æŠ¥å‘Šç”Ÿæˆå™¨ ====================

class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_text_report(result: AnalysisResult) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        lines = []
        lines.append("=" * 70)
        lines.append(f"[*] æ–‡ä»¶åˆ†ææŠ¥å‘Š")
        lines.append("=" * 70)
        lines.append(f"[-] åˆ†æç›®å½•: {result.target_path}")
        lines.append(f"[-] æ‰«ææ—¶é—´: {result.scan_time.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"[-] æ€»æ–‡ä»¶æ•°: {result.total_files:,}")
        
        # è®¡ç®—äººç±»å¯è¯»çš„å¤§å°
        total_size_human = ReportGenerator._human_size(result.total_size)
        lines.append(f"[-] æ€»å¤§å°: {total_size_human}")
        lines.append("")
        
        # æ–‡ä»¶åˆ†ç±»
        lines.append("[*] æ–‡ä»¶åˆ†ç±»ç»Ÿè®¡:")
        for category, count in sorted(result.files_by_category.items()):
            percentage = (count / result.total_files * 100) if result.total_files > 0 else 0
            bar = ReportGenerator._create_bar(percentage, width=20)
            lines.append(f"  {category:12s}: {count:6d} ({percentage:5.1f}%) {bar}")
        lines.append("")
        
        # æ‰©å±•åTOP 10
        lines.append("[*] æ–‡ä»¶æ‰©å±•å TOP 10:")
        top_ext = sorted(result.files_by_extension.items(), 
                        key=lambda x: x[1], reverse=True)[:10]
        for ext, count in top_ext:
            percentage = (count / result.total_files * 100) if result.total_files > 0 else 0
            bar = ReportGenerator._create_bar(percentage, width=20)
            lines.append(f"  {ext:10s}: {count:6d} ({percentage:5.1f}%) {bar}")
        lines.append("")
        
        # ä»£ç ç»Ÿè®¡
        code_stats = result.to_dict()['code_stats']
        lines.append("[*] ä»£ç ç»Ÿè®¡:")
        lines.append(f"  ä»£ç æ–‡ä»¶æ•°: {code_stats['total_code_files']}")
        lines.append(f"  æ€»ä»£ç è¡Œ: {code_stats['total_code_lines']:,}")
        lines.append("  ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in sorted(code_stats['languages'].items(), 
                                 key=lambda x: x[1], reverse=True)[:10]:
            bar = ReportGenerator._create_bar(
                count / code_stats['total_code_files'] * 100 if code_stats['total_code_files'] > 0 else 0,
                width=20
            )
            lines.append(f"    {lang:12s}: {count:4d} {bar}")
        lines.append("")
        
        # é‡å¤æ–‡ä»¶
        if result.duplicate_files:
            lines.append("[!] é‡å¤æ–‡ä»¶:")
            seen = set()
            for base, dup in result.duplicate_files[:10]:
                key = (base.path, base.hash)
                if key not in seen:
                    seen.add(key)
                    dup_count = len([d for d in result.duplicate_files 
                                   if d[0].path == base.path])
                    lines.append(f"  {dup_count} ä¸ªå‰¯æœ¬:")
                    lines.append(f"    [-] {base.path}")
                    lines.append(f"    [-] {ReportGenerator._human_size(base.size)}")
                    for _, dup_file in [d for d in result.duplicate_files 
                                       if d[0].path == base.path][:3]:
                        lines.append(f"    [-] {dup_file[1].path}")
                    if dup_count > 3:
                        lines.append(f"    ... è¿˜æœ‰ {dup_count - 3} ä¸ª")
            lines.append(f"æ€»è®¡é‡å¤æ–‡ä»¶ç»„: {len(result.duplicate_files)} ç»„")
        else:
            lines.append("[+] æœªå‘ç°é‡å¤æ–‡ä»¶")
        lines.append("")
        
        # æœ€å¤§æ–‡ä»¶
        if result.top_largest_files:
            lines.append("[*] æœ€å¤§çš„ 10 ä¸ªæ–‡ä»¶:")
            for i, file_info in enumerate(result.top_largest_files[:10], 1):
                human_size = ReportGenerator._human_size(file_info.size)
                lines.append(f"  {i:2d}. {human_size:>10s} - {file_info.path.name}")
        
        lines.append("=" * 70)
        return "\n".join(lines)
    
    @staticmethod
    def _human_size(size_bytes: int) -> str:
        """å°†å­—èŠ‚æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"
    
    @staticmethod
    def _create_bar(percentage: float, width: int = 20) -> str:
        """åˆ›å»ºè¿›åº¦æ¡ï¼ˆä½¿ç”¨ASCIIå­—ç¬¦ï¼‰"""
        filled = int(width * percentage / 100)
        bar = '=' * filled + '-' * (width - filled)
        return f"[{bar}]"


# ==================== ä¸»ç¨‹åº ====================

def setup_logging(verbose: bool = False) -> logging.Logger:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = logging.DEBUG if verbose else logging.INFO
    
    logger = logging.getLogger('SmartFileAnalyzer')
    logger.setLevel(level)
    
    # æ¸…é™¤å·²æœ‰å¤„ç†å™¨
    logger.handlers = []
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console = logging.StreamHandler()
    console.setLevel(level)
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console.setFormatter(formatter)
    logger.addHandler(console)
    
    return logger


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½æ–‡ä»¶åˆ†æå™¨ - ç»¼åˆåˆ†æç›®å½•ç»“æ„ä¸æ–‡ä»¶ç»Ÿè®¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s .                    # åˆ†æå½“å‰ç›®å½•
  %(prog)s /path/to/dir --json  # ç”ŸæˆJSONæŠ¥å‘Š
  %(prog)s --config config.json # ä½¿ç”¨é…ç½®æ–‡ä»¶
  %(prog)s --exclude-dir node_modules,__pycache__
        """
    )
    
    parser.add_argument('path', nargs='?', default='.',
                       help='è¦åˆ†æçš„ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰')
    
    parser.add_argument('-o', '--output',
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸ºæ ‡å‡†è¾“å‡ºï¼‰')
    
    parser.add_argument('-j', '--json', action='store_true',
                       help='è¾“å‡ºJSONæ ¼å¼æŠ¥å‘Š')
    
    parser.add_argument('--config', type=Path,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    
    parser.add_argument('--exclude-dir', type=str,
                       help='é¢å¤–æ’é™¤çš„ç›®å½•ï¼ˆé€—å·åˆ†éš”ï¼‰')
    
    parser.add_argument('--exclude-ext', type=str,
                       help='é¢å¤–æ’é™¤çš„æ‰©å±•åï¼ˆé€—å·åˆ†éš”ï¼‰')
    
    parser.add_argument('--min-size', type=int, default=0,
                       help='æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œå°äºæ­¤å€¼çš„æ–‡ä»¶å°†è¢«å¿½ç•¥')
    
    parser.add_argument('--no-duplicates', action='store_true',
                       help='ç¦ç”¨é‡å¤æ–‡ä»¶æ£€æµ‹')
    
    parser.add_argument('--workers', type=int, default=4,
                       help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰')
    
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    parser.add_argument('--version', action='version', version='%(prog)s 1.0.0')
    
    return parser.parse_args()


def main() -> int:
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    logger = setup_logging(args.verbose)
    
    try:
        # åŠ è½½é…ç½®
        config_path = args.config or Path('analyzer_config.json')
        config = Config.from_file(config_path)
        
        # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        if args.exclude_dir:
            extra_dirs = [d.strip() for d in args.exclude_dir.split(',')]
            config.exclude_dirs.extend(extra_dirs)
        
        if args.exclude_ext:
            extra_exts = [e.strip() for e in args.exclude_ext.split(',')]
            config.exclude_extensions.extend(extra_exts)
        
        config.min_file_size = args.min_size
        config.max_workers = args.workers
        config.enable_duplicate_detection = not args.no_duplicates
        
        # åˆ›å»ºåˆ†æå™¨
        target_path = Path(args.path).resolve()
        analyzer = SmartFileAnalyzer(config, logger)
        
        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze(target_path)
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.json:
            report = json.dumps(result.to_dict(), indent=2, ensure_ascii=False)
        else:
            report = ReportGenerator.generate_text_report(result)
        
        # è¾“å‡ºæŠ¥å‘Š
        if args.output:
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        else:
            print(report)
        
        # ä¿å­˜é…ç½®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not config_path.exists():
            config.save(config_path)
            logger.debug(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_path}")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 130
    except FileNotFoundError as e:
        logger.error(f"è·¯å¾„é”™è¯¯: {e}")
        return 1
    except Exception as e:
        logger.exception(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())